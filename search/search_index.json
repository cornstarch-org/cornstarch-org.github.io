{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cornstarch","text":"Cornstarch Build, Train, and Run Your Own Multimodal Model <p>Cornstarch is a multimodal model training framework that allows you to create your own multimodal model from a set of HuggingFace unimodal models, train, or use it.</p>"},{"location":"#documentation-organization","title":"Documentation Organization","text":"<ul> <li>Getting Started: Instructions on installation and setup.</li> <li>Creating a multimodal LLM: How to create a multimodal LLM from unimodal models.</li> <li>Preprocessing multimodal inputs: How to preprocess muiltimodal inputs to run a multimodal LLM.</li> <li>Training a multimodal LLM: How to train a multimodal LLM using Cornstarch.</li> <li>Multimodal LLM Parallelization: How to parallelize a multimodal LLM training.</li> </ul>"},{"location":"#research-works","title":"Research Works","text":"<p>Below is a list of works powered by Cornstarch.</p> <ul> <li>Oobleck: resilient distributed training using pipeline template</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>Insu Jang (insujang@umich.edu)</p>"},{"location":"getting_started/installation/","title":"Installing Cornstarch","text":"<p>Cornstarch is a Python library that works with Pytorch (&gt;=2.5) and ColossalAI (&gt;=0.4.4).</p>"},{"location":"getting_started/installation/#requirments","title":"Requirments","text":"<ul> <li>OS: Linux.</li> <li>Python 3.10 and above.</li> <li>Pytorch 2.5 and above.</li> <li>Ampere or Hopper GPUs. Does not guarantee working on other generations of NVIDIA GPUs or AMD GPUs.</li> </ul> <p>Note</p> <p>It is highly recommended to use a Pytorch Docker container that provides pre-compiled CUDA, NCCL, Python, and Pytorch, and then install <code>colossalai</code> and <code>Cornstarch</code> in it. For systems powered by ARM CPUs (e.g. Altera or Grace), use an NVIDIA NGC Pytorch Docker container.</p> <p>Follow NVIDIA Container toolkit guideline to use GPUs inside a Docker container.</p>"},{"location":"getting_started/installation/#install-cornstarch-from-pypi","title":"Install Cornstarch from PyPI","text":"<p>Cornstarch relies on Colossal-AI for training a multimodal LLM, which has a strict version requirement on Pytorch and Huggingface transformers that are not compatible with Cornstarch dependencies.</p> <p>For this reason, ColossalAI should first manually be installed and then should Cornstarch be installed:</p> <pre><code>$ pip install --no-deps colossalai==0.4.6\n$ pip install cornstarch\n</code></pre> <p>Note</p> <p>You will see an error message from pip dependency resolver similar to the following:</p> <pre><code>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncolossalai 0.4.6 requires torch&lt;=2.4.1,&gt;=2.2.0, but you have torch &lt;your_version&gt; which is incompatible.\ncolossalai 0.4.6 requires transformers==4.39.3, but you have transformers &lt;your_version&gt; which is incompatible.\n</code></pre> <p>But this is expected and there is no problem in installing and using cornstarch. You can also see that packages including cornstarch are successfully installed right after the error message: <pre><code>Successfully installed ... cornstarch-0.0.5 ... tokenizers-0.21.0 transformers-4.49.0 ...\n</code></pre></p>"},{"location":"getting_started/installation/#install-cornstarch-from-source","title":"Install Cornstarch from Source","text":"<p>You can also install Cornstarch from source by cloning the Github repository. Manual <code>colossalai</code> installation introduced above should still be done.</p> <pre><code>$ pip install --no-deps colossalai==0.4.6\n$ git clone https://github.com/cornstarch-org/Cornstarch\n$ cd Cornstarch\n$ pip install -e .[dev]\n</code></pre>"},{"location":"parallelization/","title":"Distributed Training Overview","text":"<p>Multimodal LLMs made by Cornstarch can be parallelized using either PyTorch Data Parallel (DDP or FSDP), ColossalAI plugins (for  tensor parallelism only), or Cornstarch plugins (for 4D parallelism).</p> <ul> <li>Using DDP/FSDP</li> <li>Cornstarch 4D Parallelism</li> </ul>"},{"location":"parallelization/cornstarch_parallel/","title":"Using Cornstarch 5D Parallelism","text":"<p>Info</p> <p>Cornstarch repository provides an end-to-end example in <code>examples/distributed/run_vlm_hybrid.py</code>.</p> <p>Using FSDP/DDP only may not be scalable depending on the infrastructure (e.g. slow inter-node networking). Cornstarch provides 5D parallelism (data parallelism, tensor parallelism, pipeline parallelism, context parallelism, and modality parallelism).</p>"},{"location":"parallelization/cornstarch_parallel/#creating-multimodalparallelplugin","title":"Creating <code>MultimodalParallelPlugin</code>","text":"<p>Cornstarch allows per-modality parallelization specification using modular information in <code>MultimodalModel</code>. Recall that a <code>MultimodalModel</code> is organized with multiple <code>ModalEncoderModule</code>s, one per modality encoder:</p> <pre><code>from cornstarch.models.multimodal_language_model import ModalEncoderModule, MultimodalModel\n\nvision_encoder = ...\naudio_encoder = ...\nllm = ...\n\nmllm = MultimodalModel(\n    encoders={\n        \"vision\": ModalEncoderModule(vision_encoder),\n        \"audio\": ModalEncoderModule(audio_encoder),\n    },\n    language_model=llm,\n)\n</code></pre> <p>Cornstarch provides the same architecture to specify parallelization per modality encoder and llm:</p> <pre><code>from cornstarch.plugin.multimodal_parallel_plugin import ModalParallelPlugin, MultimodalParallelPlugin\nfrom colossalai.booster import Booster\n\nvision_encoder_plugin = ModalParallelPlugin(...)\naudio_encoder_plugin = ModalParallelPlugin(...)\nlanguage_model_plugin = ModalParallelPlugin(...)\n\nmllm_plugin = MultimodalParallelPlugin(\n    encoder_plugins={\n        \"vision\": vision_encoder_plugin,\n        \"audio\": audio_encoder_plugin,\n    },\n    language_model_plugin=language_model_plugin,\n    ...\n)\n\n# Parallelize the model.\nbooster = Booster(plugin=mllm_plugin)\nparallel_mllm, _* = booster.boost(model=mllm, ...)\n</code></pre> <p>Note</p> <p>All encoders defined when creating <code>MultimodalModel</code> should have its corresponding plugin, otherwise an exception will be raised during parallelization.</p> <p>Note</p> <p>Parallelization is done lazily; the model is not parallelized until <code>colossalai.booster.Booster.boost()</code> is called.</p> <p>Note</p> <p>Currently using <code>MultimodalParallelPlugin</code> forces to use pipeline parallelism, as encoders and the LLM should be pipelined in different stages.</p> <p>The structure of <code>MultimodalParallelPlugin</code> exactly follows that of <code>MultimodalModel</code>. Each encoder and the language model must have their own <code>ModalParallelPlugin</code>, which specifies how each modality should be parallelized.</p>"},{"location":"parallelization/cornstarch_parallel/#specifying-parallelization","title":"Specifying Parallelization","text":"<p>Each <code>ModalParallelPlugin</code> has four arguments for parallel configurations: <code>tp_size</code>, <code>sp_size</code>, <code>sequence_parallelism_mode</code>, and <code>pipeline_template</code>. The arguments are mapped to the following three parallel dimensions:</p> <ul> <li>Tensor Parallelism (TP): <code>tp_size</code></li> <li>Context Parallelism (CP): <code>sp_size</code>, <code>sequence_parallelism_mode</code>, and <code>context_parallel_distribution_mode</code>.</li> <li>Pipeline Parallelism (PP): <code>pipeline_template</code></li> </ul> <p>Note</p> <p>Cornstarch uses the term <code>sequence_parallelism</code> for backward compatibility: which is used by colossalai. Cornstarch does not support Megatron's sequence parallelism that is used as a combination of tensor parallelism.</p>"},{"location":"parallelization/cornstarch_parallel/#tensor-parallelism","title":"Tensor Parallelism","text":"<p>All embedding and linear layers are partitioned to tensor parallel ranks. For attention layers, it is partitioned in head dimension; the number of heads of a model should be divisible to <code>tp_size</code>.</p> <p>Note</p> <p>Currently specifying different number of <code>tp_size</code> to different encoders or LLM is not supported.</p>"},{"location":"parallelization/cornstarch_parallel/#context-parallelism","title":"Context Parallelism","text":"<p>Cornstarch supports Ulysses all-to-all style context parallelism and Llama context parallelism (head-by-head parallelism). You can set <code>sequence_parallelism_mode</code> to <code>all_to_all</code> (Ulysses) or <code>ring_attn</code> (llama CP) to choose the context parallelism mechanism.</p> <p>Encoders and the LLM can have different number of <code>sp_size</code>.</p> <p>Note</p> <p>If <code>sp_size &lt;= 1</code>, <code>sequence_parallelism_mode</code> is ignored.</p> <p>Note</p> <p>Currently context parallelism is supported only for the LLM.</p> <p>Context parallelism split sequeneces into subsets of tokens and distribute them into multiple ranks. There are multiple ways of partitioning sequences and distributing tokens into ranks that Corntarch supports:</p> <ul> <li><code>uniform</code>: The simplest way of such partitioning. Chunk every sequence into <code>cp_world_size</code> chunks, and each rank takes one portion.</li> <li><code>zigzag</code>: For causal attention, the amount of computation becomes imbalanced if tokens are uniformly distributed. <code>zigzag</code> partitions the sequence into <code>2 * cp_world_size</code> and each rank gets one portion from the upper half and another from the lower half, the workload sum of which is always balanced in causal attention.</li> <li><code>makespan_min</code>: In multimodal LLM, attention should no longer be simple causal; vision tokens should attend each other regardless of their location (previous vision tokens can attend to the future vision tokens). In this form of attention, zigzag is no longer be balanced, <code>makespan_min</code> computes the amount of workloads per token block (128 tokens per block) and distributes them to minimize overall makespan (execution time). The number of tokens per rank may be different depending on the amount of workloads.</li> </ul> <p></p> <p>The token distribution scheme can be configured by passing <code>cornstarch.shardformer.shard.shard_config.ContextParallelDistributionMode.[UNIFORM | ZIGZAG | MAKESPAN_MIN]</code> to LLM's <code>shard_config.context_parallel_distribution_mode</code>. Default is <code>MAKESPAN_MIN</code>.</p>"},{"location":"parallelization/cornstarch_parallel/#pipeline-parallelism","title":"Pipeline Parallelism","text":"<p>Cornstarch uses pipeline template to specify pipeline parallelism (adopted from Oobleck), instead of simply having the number of pipeline stages, to let users to specify pipeline stages more freely.</p> <p>A way of creating a pipeline template is as follows.</p> <ol> <li>Get all layers required to be included in a template.</li> <li>Split the layers into a list of sublayers properly, each of which will be a set of layers of a pipeline stage.</li> <li>Create a <code>cornstarch.pipeline_template.PipelineTemplate</code> instance.</li> </ol> <p>For HF models, Cornstarch provides a way of automatically getting all layers in a model:</p> Getting layers from a HF model<pre><code>from cornstarch.pipeline_template import PipelineTemplate\nfrom transformers.models.llama import LlamaForCausalLM\n\nlanguage_model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\nlayers: list[str] = PipelineTemplate.get_modules(language_model)\n\n# layers: [\"model.embed_tokens\",\n#   \"model.layers.0\",\n#   \"model.layers.1\",\n#   ...\n#   \"model.layers.31\",\n#   \"model.norm\",\n#   \"lm_head\"]\n</code></pre> <p>Split the list of layers to however you want as <code>list[list[str]]</code>. For example, If you want to make a 2-stage pipeline template,</p> <pre><code>layers_per_stage = [\n    layers[:17],\n    layers[17:],\n]\n</code></pre> <p>which will assign the <code>embed_tokens</code> layer and first 16 decoder layers to the first pipeline stage, all the others to the second pipeline stage.</p> <p>Now create a pipeline template:</p> <pre><code>pipeline_template = PipelineTemplate(\n    model_name=PipelineTemplate.get_model_name(language_model),\n    modules_per_stage=layers_per_stage,\n)\n</code></pre> <p>which will give you a 2-stage pipeline template for <code>LlamaForCausalLM</code>: <pre><code>pipeline_template\nPipelineTemplate(transformers.models.llama.modeling_llama.LlamaForCausalLM, 2 stages)\n</code></pre></p> <p>Note</p> <p>Cornstarch verifies if the pipeline template is for the given unimodal model by checking its name and modules_per_stage. It will raise an exception if a pipeline template for different model is given.</p>"},{"location":"parallelization/cornstarch_parallel/#data-parallelism","title":"Data Parallelism","text":"<p>Data Parallelism is not explictly specified by some arguments. Instead, Cornstarch automatically infers how many data parallel replicas are needed by computing the number of ranks in a parallel multimodal LLM and divide the world size by it.</p> <p>An example of parallelization<pre><code>vision_encoder_plugin = ModalParallelPlugin(\n    tp_size=2, sp_size=1,\n    pipeline_template= # a pipeline template with 1 stage\n)\naudio_encoder_plugin = ModalParallelPlugin(\n    tp_size=1, sp_size=1,\n    pipeline_template= # a pipeline template with 1 stage\n)\nlanguage_model_plugin = ModalParallelPlugin(\n    tp_size=4, sp_size=2,\n    pipeline_template= # a pipeline template with 3 stages\n)\n\nmllm_plugin = MultimodalParallelPlugin(\n    encoder_plugins={\n        \"vision\": vision_encoder_plugin,\n        \"audio\": audio_encoder_plugin,\n    },\n    language_model_plugin=language_model_plugin,\n    ...\n)\n</code></pre> The number of total ranks in the example above is 27 (2*1*1 + 1*1*1 + 4*2*3). If 54 GPUs join the training, there will be 2 data parallel replicas.</p> <p>Note</p> <p>Cornstarch does not optimize rank assignment and leaves it to user responsibility. The example above assigns 3 GPUs to the encoders and 8 GPUs to the LLM; if each node has 8 GPUs, cross-node GPUs may be assigned to the LLM (5 GPUs from one node, and 3 GPUs from another one).</p>"},{"location":"parallelization/cornstarch_parallel/#modality-parallelism","title":"Modality Parallelism","text":"<p>Cornstarch executes multiple modality encoders in parallel, as there is no dependency between them. By using <code>cornstarch.plugin.multimodal_parallel_plugin.MultimodalParallelPlugin</code>, modality encoders will be assigned to different devices and executed separately.</p> <p>If you do not want to parallelize them, consider using <code>cornstarch.plugin.encoders_colocated_plugin.EncodersColocatedPlugin</code>, which colocates multiple modality encoders into the same GPUs and execute them sequentially.</p>"},{"location":"parallelization/cornstarch_parallel/#running-parallelized-module","title":"Running Parallelized Module","text":"<p>Pipeline parallelism interleaves forward passes and backward passes; therefore existing code for training (<code>loss = model(**inputs); loss.backward()</code>) is not compatible. You have to use <code>Booster.execute_pipeline()</code> API to run the model:</p> <pre><code>outputs = booster.execute_pipeline(\n    dataloader_iterator,\n    model,\n    crierion,\n    optimizer,\n    return_loss=True,\n    return_outputs=False,\n)\n\noptimizer.step()\noptimizer.zero_grad()\n</code></pre> <p>Refer to Colossal-AI Booster API and examples for more details about the arguments.</p>"},{"location":"parallelization/ddp_fsdp/","title":"Using DDP/FSDP","text":"<p>Info</p> <p>Cornstarch repository provides an end-to-end example in <code>examples/distributed/run_vlm_ddp.py</code> and <code>examples/distributed/run_vlm_fsdp.py</code>.</p> <p>PyTorch DDP and FSDP work by simply wrapping the original model with the API. This design principle is also compatible with Cornstarch multimodal LLM, therefore DDP/FSDP can be used with Cornstarch.</p> An example of using PyTorch DDP<pre><code>import torch\nimport torch.distributed as dist\nfrom torch.optim.adam import Adam\nfrom torch.nn.parallel import DistributedDataParallel\nfrom tqdm import tqdm\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    PreTrainedModel,\n    AutoTokenizer,\n)\nfrom transformers.models.clip import CLIPVisionModel, CLIPImageProcessor\n\nfrom cornstarch.models.multimodal_language_model import (\n    ModalEncoderModule,\n    MultimodalModel,\n    MultimodalModelProcessor,\n)\n\n# Create a mllm\nvision_model_name = \"openai/clip-vit-base-patch32\"\nlanguage_model_name = \"meta-llama/Llama-3.2-1B\"\nvision_encoder = CLIPVisionModel.from_pretrained(vision_model_name)\nlanguage_model = AutoModelForCausalLM.from_pretrained(language_model_name)\n\nmodel = MultimodalModel(\n    encoders={\"vision\": vision_encoder},\n    language_model=language_model,\n).to(dtype=torch.bfloat16, device=\"cuda\")\n\n# Create a processor\nimage_processor = CLIPImageProcessor.from_pretrained(vision_model_name)\ntext_tokenizer = AutoTokenizer.from_pretrained(language_model_name, use_fase=True)\ntext_processor.pad_token_id = text_processor.eos_token_id\n\nprocessor = MultimodalModelProcessor(\n    tokenizer=text_tokenizer,\n    image_processor=image_processor,\n)\n\n# Parallelize the model\ndist.init_process_group()\nddp_model = DistributedDataParallel(model)\noptimizer = Adam(ddp_model.parameters())\n\noutputs = ddp_model(**inputs)\nloss = outputs.loss\nloss.backward()\n\noptimizer.step()\noptimizer.zero_grad()\n</code></pre> <p>Similarly, FSDP can be used by wrapping the model with <code>torch.distributed._composable.fsdp.fully_shard()</code>:</p> <p>Note</p> <p>It is very important to properly define a wrapping unit using <code>ModuleWrapPolicy</code> (FSDP1) or <code>fully_shard</code> (FSDP2) in performance and correctness. Parameters with different <code>requires_grad</code> cannot be wrapped together, thus they need to be wrapped in a different group; otherwise it will raise an error. Because of this, using FSDP still requires knowledge of model internal architecture.</p> An example of using PyTorch FSDP1<pre><code>from torch.distributed.fsdp import FullyShardedDataParallel\nfrom torch.distributed.fsdp.wrap import ModuleWrapPolicy\n# All the same before dist.init_process_group()\n\ndist.init_process_group()\n\nfsdp_model = FullyShardedDataParallel(\n    module=model,                           # required\n    auto_wrap_policy=ModuleWrapPolicy(      # required\n        [\n            ModalEncoderModule,\n            MultimodalProjector,\n            torch.nn.Embedding,\n            CLIPEncoderLayer,               \n            LlamaDecoderLayer,\n        ]\n    ),\n    sharding_strategy=ShardingStrateegy.FULL_SHARD,  # optional\n    cpu_offload=CPUOffload(),                        # optional\n    backward_prefetch=BackwardPrefetch.BACKWARD_PRE, # optional\n    forward_prefetch=True,                           # optional\n)\noptimizer = Adam(fsdp_model.parameters())\n\noutputs = fsdp_model(**inputs)\nloss = outputs.loss\nloss.backward()\n\noptimizer.step()\noptimizer.zero_grad()\n</code></pre> An example of using PyTorch FSDP2<pre><code>from torch.distributed._composable.fsdp import fully_shard\n\nvision_encoder = CLIPVisionModel.from_pretrained(vision_model_name)\nlanguage_model = AutoModelForCausalLM.from_pretrained(language_model_name)\n\n# The location of fully_shard() for subgroups does not have to be here.\nfor layer in vision_encoder.vision_model.encoder.layers:\n    fully_shard(layer)\nfully_shard(vision_encoder.vision_model)\nfor layer in language_model.model.layers:\n    fully_shard(layer)\n\nmodel = MultimodalModel(\n    encoders={\"vision\": vision_encoder},\n    language_model=language_model,\n).to(dtype=torch.bfloat16, device=\"cuda\")\nfully_shard(model.vision_encoder.projector)\nfsdp_model = fully_shard(model)\n\ndist.init_process_group()\n\noptimizer = Adam(fsdp_model.parameters())\n\noutputs = fsdp_model(**inputs)\nloss = outputs.loss\nloss.backward()\n\noptimizer.step()\noptimizer.zero_grad()\n</code></pre>"},{"location":"using_cornstarch/creating_mllm/","title":"Creating a MLLM model","text":"<p>Info</p> <p>Cornstarch repository provides an end-to-end example in <code>examples/pretrain_vlm.py</code>.</p>"},{"location":"using_cornstarch/creating_mllm/#creating-a-multimodal-llm","title":"Creating a Multimodal LLM","text":"<p>Cornstarch supports creating a modular multimodal LLM from HuggingFace models. For example, you can create a vision-language model (VLM) with Llama 8b and ViT as follows:</p> <pre><code>from transformers.models.llama.modeling_llama import LlamaForCausalLM\nfrom transformers.models.vit.modeling_vit import ViTPreTrainedModel\nfrom cornstarch.models.multimodal_language_model import ModalEncoderModule, MultimodalModel\n\nllm = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\nvision_encoder = ViTPreTrainedModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nmllm = MultimodalModel(\n    encoders={\n        \"vision\": ModalEncoderModule(vision_encoder),\n    },\n    language_model=llm,\n)\n</code></pre> <p>where <code>mllm</code> has <code>language_model</code> and <code>vision_encoder</code> modules.</p>"},{"location":"using_cornstarch/creating_mllm/#model-architecture","title":"Model Architecture","text":"<p>A simplified <code>MultimodalModel</code> architecture is as follows:</p> <pre><code>cornstarch.MultimodalModel\n\u251c\u2500\u2500 vision_encoder (cornstarch.ModalEncoderModule)\n\u2502   \u251c\u2500\u2500 module (transformers.PreTrainedModel)\n\u2502   \u2514\u2500\u2500 projector (cornstarch.MultimodalProjector)\n\u251c\u2500\u2500 audio_encoder (cornstarch.ModalEncoderModule)\n\u2502   \u251c\u2500\u2500 module (transformers.PreTrainedModel)\n\u2502   \u2514\u2500\u2500 projector (cornstarch.MultimodalProjector)\n\u251c\u2500\u2500 whatever_encoder (cornstarch.ModalEncoderModule)\n\u2502   \u251c\u2500\u2500 module (transformers.PreTrainedModel)\n\u2502   \u2514\u2500\u2500 projector (cornstarch.MultimodalProjector)\n\u2514\u2500\u2500 language_model (transformers.PreTrainedModel)\n</code></pre> <p>It has one <code>language_model</code> that represents the base LLM from HuggingFace <code>transformers</code>, and can have arbitrary number of encoders. Encoders are stored as a dictionary in <code>MultimodalModel.encoders</code> where a key represents its encoder name (<code>vision</code>, <code>audio</code>, or <code>whatever</code> in the example above) and the corresponding value is a <code>ModalEncoderModule</code>.</p> <p><code>ModalEncoderModule</code> is a single modality encoder that includes an encoder and a projector. An encoder module is from HuggingFace <code>transformers</code>, and Cornstarch provides the definition of the projector (<code>cornstarch.models.multimodal_language_model.modeling_multimodal_language_model.MultiomdalProjector</code>).</p>"},{"location":"using_cornstarch/creating_mllm/#creating-a-projector","title":"Creating a Projector","text":"<p>Cornstarch provides two ways of creating <code>MultimodalProjector</code>.</p> <pre><code>class MultimodalProjector(PreTrainedModel):\n    def __init__(self, config: MultimodalProjectorConfig, projection: Optional[nn.Module] = None): ...\n</code></pre> <p>First, you can simply wrap your own <code>torch.nn.Module</code> with <code>MultimodalProjector</code>. When you provide your module to <code>projection</code> in creating a <code>MultimodalProjector</code> instance, Cornstarch will use the given module as a projector module.</p> <p>The generated projector module should explicitly be given to <code>ModalEncoderModule</code>.</p> <pre><code>wrapped_projector_module = MultiodalProjector(your_config, my_projector_module)\nencoder = ModalEncoderModule(module=my_encoder_module, projector=wrapped_projector_module)\n</code></pre> <p>Second, Cornstarch can automatically initialize a new projector if no projector is given in <code>ModalEncoderModule</code>:</p> <pre><code>encoder = ModalEncoderModule(module=my_encoder_module)\n# which is equivalent to \nencoder = ModalEncoderModule(module=my_encoder_module, projector=None)\n</code></pre> <p>It adopts lazy initialization; a projector module is not initialized during creating a <code>ModalEncoderModule</code>. Instead, when a <code>MultimodalModel</code> is created, it checks whether a projection module in the given <code>MultimodalProjector</code> is <code>None</code>, and creates a projector module if so.</p> <p><code>MultimodalModel</code> accepts two arguments for projector creation as you want: <code>init_projector_type</code> and <code>init_activation</code>:</p> <pre><code>class MultimodalModel(nn.Module):\n    def __init__(\n        self,\n        ...,\n        init_projector_type: str = \"linear\",\n        init_activation: str = \"gelu\",\n    ): ...\n</code></pre> <p>Currently <code>MultimodalModel</code> accepts either <code>linear</code> or <code>mlp</code> as an <code>init_projector_type</code>:</p> <ul> <li><code>linear</code>: has a single <code>torch.nn.Linear</code> layer.</li> <li><code>mlp</code>: has two <code>torch.nn.Linear</code> layers, where there is an activation layer as <code>init_activation</code> type in the middle. The type of activations that Cornstarch supports is defined in <code>transformers.activations.ACT2CLS</code>.</li> </ul>"},{"location":"using_cornstarch/creating_mllm/#callback-interface","title":"Callback Interface","text":"<p>Cornstarch provides callback interface to add multimodal specific features without modifying underlying unimodal modules. For example, Llava-Next has to choose an image feature based on its select strategy, which its base vision encoder such as <code>CLIPVisionModel</code> does not have. Callback is a great place to implement such features.</p> <p>Cornstarch provides three types of callbacks in <code>ModalEncoderModule</code>:</p> <pre><code>ModalEncoderModule(\n    model=vision_encoder,\n    projector=vision_projector,\n    preprocess_callback=preprocess_vision_callback,\n    postprocess_module_callback=postprocess_vision_callback,\n    postprocess_projector_callback=postprocess_projector_callback,\n)\n</code></pre> <p>The execution order of callbacks and modules is as follows:</p> <pre><code>encoder: ModalEncoderModule\nfor encoder in mllm.encoders:\n    preprocessed_encoder_input = encoder.preprocess_callback(encoder_input)\n    encoder_output = encoder.module(preprocessed_encoder_input)\n    postprocessed_encoder_output = encoder.postprocess_module_callback(encoder_output)\n    module_output = encoder.projector(postprocessed_encoder_output)\n    postprocessed_module_output = encoder.postprocess_projector_callback(module_output)\n\n# merge a list of postprocessed_module_outputs to text_embedding\nmerged_input = merge(postprocessed_module_outputs, language_inputs_embedding)\noutput = language_model(merged_input)\n</code></pre> <p>Inputs and outputs of each callback are as follows:</p> <ul> <li><code>preprocess_callback(inputs: dict[str, Any]) -&gt; dict[str, Any]</code>: gets the inputs of the modality encoder as a dictionary. Returns a modified dictionary which will be used as actual inputs of the modality encoder.</li> <li><code>postprocess_encoder_callback(inputs: dict[str, Any], output: BaseModelOutput | tuple) -&gt; BaseModelOutput | tuple</code>: gets the inputs and the output of the modality encoder. The output is either <code>BaseModelOutput</code> or <code>tuple</code>, depending on the encoder configuration <code>return_dict</code>. Returns a modified output which will be forwarded to a projector.</li> <li><code>postprocess_projector_callback(inputs: dict[str, Any], output: BaseModelOutput | tuple) -&gt; BaseModelOutput | tuple</code>: gets the inputs of the modality encoder, and the output of the projector. The output is either <code>BaseModelOutput</code> or <code>tuple</code>, depending on the encoder configuration <code>return_dict</code>. Returns a modified output which will be forwarded to the LLM.</li> </ul>"},{"location":"using_cornstarch/creating_mllm/#a-llava-next-example-of-utilizing-callback-interface","title":"A Llava-Next example of utilizing callback interface","text":"<p>The original <code>LlavaNextForConditionalGeneration.forward()</code> is implemented as follows:</p> <pre><code>class LlavaNextForConditionalGeneration:\n    def forward(\n        self,\n        ...\n    ):\n        ...\n\n        if inputs_embeds is not None:\n            inputs_embeds = self.get_input_embeddings()(input_ids)\n\n        image_features = None\n        if pixel_values is not None and pixel_values.size(0) &gt; 0:\n            image_features = self.get_image_features(\n                pixel_values,\n                image_sizes,\n                vision_feature_layer=vision_feature_layer,\n                vision_feature_select_strategy=vision_feature_select_strategy,\n            )\n\n            # NOTE we only support multimodal_patch_merge_type == \"spatial_unpad\"\n            image_features, feature_lens = self.pack_image_features(\n                image_features,\n                image_sizes,\n                vision_feature_select_strategy=vision_feature_select_strategy,\n                image_newline=self.image_newline,\n            )\n\n        # embed vision output result to inputs_embeds\n        n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n        n_image_features = image_features.shape[0]\n        if n_image_tokens != n_image_features:\n            raise ValueError(\n                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n            )\n        special_image_mask = (\n            (input_ids == self.config.image_token_index)\n            .unsqueeze(-1)\n            .expand_as(inputs_embeds)\n            .to(inputs_embeds.device)\n        )\n        image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n\n        outputs = self.language_model(...)\n        ...\n</code></pre> <p>The highlighted Llava-next specific feature can be implemented in a callback:</p> <p><pre><code>from typing import Optional\nfrom transformers.modeling_outputs import BaseModelOutput, ModelOutput\n\ndef postprocess_projector_callback(\n    inputs: dict,\n    output: BaseModelOutput | tuple,\n    vision_feature_select_strategy: Optional[str] = None,\n) -&gt; BaseModelOutput | tuple:\n    pixel_values = inputs.get(\"pixel_values\", None)\n\n    if pixel_values is not None and pixel_values.size(0) &gt; 0:\n        # output[0] == output.last_hidden_state\n        image_features = output[0]\n\n        # pack_image_features function should be borrowed from\n        # the LlavaNextForConditionalGeneration class\n        image_features, feature_lens = pack_image_features(\n            image_features,\n            image_sizes,\n            vision_feature_select_strategy=vision_feature_select_strategy,\n            image_newline=image_newline,\n        )\n\n    # replace output hidden state with postprocessed results\n    if isinstance(output, ModelOutput):\n        output.last_hidden_state = image_features\n    else:\n        output = (image_features,) + output[1:]\n\n    return output\n</code></pre> which can be used for any combination of a vision encoder and an LLM:</p> Llava-Next with CLIP+Mistral using Cornstarch<pre><code>clip = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nmistral = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n\nmllm = MultimodalModel(\n    encoders={\n        \"vision\": ModalEncoderModule(\n            model=clip,\n            postprocess_projector_callback=postprocess_projector_callback,\n        )\n    },\n    language_model=mistral,\n)\n</code></pre> Llava-Next with Siglip+Llama using Cornstarch<pre><code>siglip = SiglipVisionModel.from_pretrained(\"google/siglip-so400m-patch14-384\")\nllama = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n\nmllm = MultimodalModel(\n    encoders={\n        \"vision\": ModalEncoderModule(\n            model=siglip,\n            postprocess_projector_callback=postprocess_projector_callback,\n        )\n    },\n    language_model=llama,\n)\n</code></pre>"},{"location":"using_cornstarch/preprocessing_inputs/","title":"Preprocessing inputs","text":"<p>Info</p> <p>Cornstarch repository provides an end-to-end example in <code>examples/pretrain_vlm.py</code>.</p> <p>Running a multimodal model needs multimodal inputs. Yet processors for each modality model do not have interaction required for multimodal model execution.</p> <p>In this section, we introduce the basics of multimodal interaction and Cornstarch APIs for interaction.</p>"},{"location":"using_cornstarch/preprocessing_inputs/#interation-between-modality-inputs","title":"Interation between modality inputs","text":"<p>Multimodal LLMs merge modality encoder outputs into text embedidng and execute an LLM together. In the text input, it is typical to use special tokens such as <code>&lt;image&gt;</code> to indicate this is where modality encoder outputs should be located:</p> <pre><code>from transformers.models.llava_next import LlavaNextProcessor\n\nprocessor = LlavaNextProcessor.from_pretrained(\"llava-hf/llama3-llava-next-8b-hf\")\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\ntext = processor.apply_chat_template(messages, return_tensors=\"pt\")\n\n# text:\n# '&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n&lt;image&gt;\\nDescribe this image.&lt;|eot_id|&gt;'\n</code></pre> <p>When this text is tokenized, the <code>&lt;image&gt;</code> token is replaced with multiple image token IDs as placeholders, where later modality encoder outputs are injected by replacing the hidden states of the corresponding location.</p> <pre><code>inputs = processor(images=image, text=text, return_tensors=\"pt\")\n\n# inputs[\"input_ids\"]:\n# tensor([128000, 128006,    882, 128007,    271, 128256, 128256, 128256, 128256,\n#         128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n#         128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n#         ...])\n# token 128256 (image special token) is repetitively added to the tokenized input_ids,\n# where the number of special tokens is exactly the same with the number of image tokens\n# that will be generated after executing the vision encoder.\n</code></pre>"},{"location":"using_cornstarch/preprocessing_inputs/#cornstarch-multimodalprocessor","title":"Cornstarch <code>MultimodalProcessor</code>","text":"<p>To support the same feature with multimodal-unaware processors, Cornstarch provides <code>MultimodalProcessor</code> class to define the multimodal interaction between processors, feature extractors, and a tokenizer.</p> <pre><code>from cornstarch.models.multimodal_language_model.processing_multimodal_language_model import MultimodalProcessor\n\nmm_processor = MultimodalProcessor(\n    encoder_processors={\n        \"vision\": CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch16\"),\n        \"audio\": WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v3\"),\n    },\n    llm_tokenizer=LlamaTokenizerFast.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\"),\n    num_feature_calculation_funcs={},\n)\n</code></pre> <p>which wraps a CLIP image processor for <code>vision</code> modality encoder, a Whisper feature extractor for <code>audio</code> modality encoder, and a Llama tokenizer for LLM into a single processor.</p>"},{"location":"using_cornstarch/preprocessing_inputs/#functions-for-calculating-the-number-of-features","title":"Functions for calculating the number of features","text":"<p>In the example of using llava processor above, tokenized inputs has a lot of image tokens (128256). The number of the image tokens must be exactly the same with the number of image features, otherwise merging the modality encoder outputs fails:</p> <pre><code>(inputs[\"input_ids\"] == processor.tokenizer.convert_tokns_to_ids(processor.image_token)).sum()\n# 1176 for image size 336x336\n\nimage_features.shape # during LlavaNextForConditionalGeneration.forward()\n# torch.Size([1176, 4096])\n</code></pre> <p>Because Cornstarch does not know how many tokens the given model will generate, users need to provide functions that return the number of tokens in <code>num_features_calculation_funcs</code>:</p> <pre><code>MultimodalProcessor(\n    ...\n    num_feature_calculation_funcs: {\n        \"vision\": lambda *args, **kwargs: (224 // 16) ** 2 + 1,\n        \"audio\": lambda *args, **kwargs: 1500,\n    }\n)\n</code></pre> <p>Cornstarch provides processor inputs and processor outputs as two dictionaries as the input of the function:</p> <pre><code>inputs = # input to the modality encoder processor as a dictionary\noutputs = # output of the modality encoder processor as a dictionary\nnum_features = callback(inputs, outputs)\n</code></pre> <p>where <code>num_features</code> should be either - <code>list[int]</code>: a list of the number of features, one per modality input, across the entire batch, or - <code>list[list[int]]</code>: a list of the lists of the number of features, one per modality input per batch.</p> <p>Note</p> <p>For modality encoders that Cornstarch officially supports, calculation functions are automatically set.</p> <p>However, if your multimodal model needs more features (e.g. Llava-next's dynamic high resolution that its underlying CLIP vision encoder does not support) or if you use a modality encoder that Cornstarch does not know, a custom function must be provided.</p>"},{"location":"using_cornstarch/preprocessing_inputs/#token-id-configuration","title":"Token ID configuration","text":"<p>The tokenizer does not know which special tokens should be used for modality encoders. At the same time, the LLM in <code>MultimodalModel</code> does not know which token IDs should be replaced with the modality encoder outputs when merging them, either.</p> <p>For this reason, <code>MultimodalProcessor</code>, unlike processors that are independent from models in HuggingFace transformers, requires to take <code>MultimodalModel</code> to add such interaction:</p> <pre><code>class MultimodalProcessor:\n    def __init__(\n        self,\n        ...\n        model: MultimodalModel,\n        num_features_calculation_funcs: dict[str, Callable] = {},\n        predefined_tokens: dict[str, str] = {},\n    ):\n</code></pre> <p>By default, Cornstarch registers <code>&lt;modal_key&gt;</code> special tokens to the tokenizer:</p> <pre><code>mm_processor.llm_tokenizer.special_tokens_map\n{'bos_token': '&lt;|begin_of_text|&gt;',\n 'eos_token': '&lt;|eot_id|&gt;',\n 'unk_token': '&lt;unk&gt;',\n 'additional_special_tokens': ['&lt;vision&gt;', '&lt;audio&gt;']} # Since we have \"vision\" and \"audio\" as modality keys, these two tokens are registered\n</code></pre> <p>When your dataset already includes its own special token, you can override the token by providing <code>predefined_tokens</code>. The following example registers <code>&lt;image&gt;</code> instead of <code>&lt;vision&gt;</code> for the vision encoder:</p> <pre><code>mm_processor = MultimodalProcessor(..., predefined_tokens={\"vision\": \"&lt;image&gt;\"})\n\nmm_processor.llm_tokenizer.special_tokens_map\n{'bos_token': '&lt;|begin_of_text|&gt;',\n 'eos_token': '&lt;|eot_id|&gt;',\n 'unk_token': '&lt;unk&gt;',\n 'additional_special_tokens': ['&lt;image&gt;', '&lt;audio&gt;']}\n</code></pre>"},{"location":"using_cornstarch/preprocessing_inputs/#data-preprocessing-with-multimodalprocessor","title":"Data preprocessing with <code>MultimodalProcessor</code>","text":"<p>Cornstarch designs the <code>MultimodalProcessor</code> to provide the maximum flexibility of data processing to users. To avoid duplicated arguments from multiple modalities and the LLM, <code>MultimodalProcessor</code> takes a dictionary per modality encoder and the LLM:</p> <pre><code>class MultimodalProcessor:\n    def __call__(\n        self,\n        encoder_inputs: dict[str, dict],\n        llm_inputs: dict,\n        return_tensors: str | TensorType = TensorType.PYTORCH,\n        **kwargs,\n    ) -&gt; BatchFeature:\n        ...\n</code></pre> <p>Cornstarch executes each processor and the tokenizer with the corresponding input dictionary. It also forwards arguments in <code>kwargs</code> if a processor accepts the argument. So, you do not have to repetitively include some common argument to dictionaries for multiple processors.</p>"},{"location":"using_cornstarch/training_mllm/","title":"Training a MLLM model","text":"<p>Info</p> <p>Cornstarch repository provides an end-to-end example in <code>examples/pretrain_vlm.py</code>.</p>"},{"location":"using_cornstarch/training_mllm/#training-a-multimodal-llm","title":"Training a Multimodal LLM","text":"<p><code>MultimodalModel</code> inherits from <code>torch.nn.Module</code>, which has its own <code>forward()</code> function for inference and training. You can call the model as you do with a typical <code>torch.nn.Module</code> as:</p> <pre><code>mllm = MultimodalModel(...)\noutput = mllm(**inputs)\nloss = output.loss\nloss.backward()\n# optimizer step, zero_grad, etc.\n</code></pre>"},{"location":"using_cornstarch/training_mllm/#freezing-modules","title":"Freezing Modules","text":"<p>Cornstarch supports freezing a portion of the <code>MultimodalModel</code>. For encoder (<code>ModalEncoderModule</code>), an encoder and a projector can individually be frozen:</p> <pre><code>mllm = MultimodalModule(\n    encoders={\n        \"vision\": ModalEncoderModule(...),\n        \"audio\": ModalEncoderModule(...),\n    },\n    langauge_model=llm,\n)\n\nmllm.train(\n    encoders_mode={\n        \"vision\": (False, True), # encoder and projector, respectively\n        \"audio\": (True, True),\n    },\n    llm_mode=False,\n)\n</code></pre> <p>Info</p> <p>if <code>encoders_mode</code> is not given, the train mode of all encoders including projectors' is set to <code>llm_mode</code>.</p> <pre><code>mllm.train(llm_mode=True)\n</code></pre> <p>is equivalent to:</p> <pre><code>mllm.train(\n    {encoder: (True, True) for encoder in mllm.encoders},\n    llm_mode=True,\n)\n</code></pre> <p>If the given encoder key does not exist in the <code>MultimodalModel</code>, it raises a <code>ValueError</code>. For example, if you call <code>mllm.train(\"non_existing_encoder\", mode=False)</code>, the encoder key <code>non_existing</code> does not exist in the <code>MultimodalModel</code> encoder dictionary, hence it raises an error.</p> <p>Note</p> <p>PyTorch <code>torch.nn.Module.train(mode=False)</code> API cannot be used. It still computes gradients for frozen modules and you cannot get any benefits in computing time and memory consumption.</p>"},{"location":"using_cornstarch/training_mllm/#merging-encoder-outputs-to-llm-embedding-space","title":"Merging Encoder Outputs to LLM Embedding Space","text":"<p>When multimodal LLM forward is executed, modality encoders are executed first. After that, LLM input embedding layer is executed, the modality encoder outputs is embedded into the LLM embedding space, and then execute the remaining LLM layers with modality encoder outputs and LLM embedding outputs. Cornstarch provides two ways of embedding  modality encoder outputs to LLM embedding outputs: prepending and injecting.</p>"},{"location":"using_cornstarch/training_mllm/#prepending-modality-encoder-outputs","title":"Prepending modality encoder outputs","text":"<p>Like initial VLMS (e.g. Llava 1.5), prepending simply attaches all modality encoder outputs prior to text embedding outputs. The order of prepending is the opposite to the order of modality encoders. When there are multiple modality encoders, encoders are executed in order of their order in the <code>MultimodalModule.encoders</code> dictionary. If the output of the last modality encoder is prepended first, the result will be <code>[modality_encoder1][modality_encoder2]...[modality_encoderN][llm_embedding]</code>, which is the same as the order of modality encoders in the dictionary.</p> <p>Note</p> <p>It does not require any specific LLM tokens to specify the location of modality encoders.</p>"},{"location":"using_cornstarch/training_mllm/#injecting-modality-encoder-outputs","title":"Injecting modality encoder outputs","text":"<p>Unlike simply prepending modality encoders, injecting mehcanism injects modality encoder outputs to the location that user wants to put them to. To specify where to put the modality encoder outputs, custom tokens must be added before running the model. Use <code>MultimodalModel.set_token_ids()</code> API to register the token IDs per modality encoders:</p> <pre><code>mllm = MultimodalModel(\n    encoders={\n        \"vision\": ...,\n        \"audio\": ...,\n    },\n    language_model=llm,\n)\n\nmllm.set_token_ids({\n    \"vision\": vision_token_id,\n    \"audio\": audio_token_id,\n})\n</code></pre> <p>The model is automatically switched to injection mode and registered token IDs are used in merging modality encoder outputs.</p> <p>It is a user responsibility to prepare placeholders that modality encoder outputs will replace. During injection, Cornstarch replaces embeddings in the indices of the corresponding token IDs with the modality encoder outputs. For example, if the vision token ID is 200 and the given <code>input_ids</code> (tokenized text) looks like:</p> <p><pre><code>input_ids = [1, 33, 7752, 10452, 200, 200, 200, 200, 4465, 4832, 6921, ...]\n</code></pre> You are expecting there would be 4 vision tokens from the vision encoder, which will be injected at indices 4, 5, 6, and 7 (zero-based).</p> <p>Note</p> <p>The number of tokens must match.</p>"}]}